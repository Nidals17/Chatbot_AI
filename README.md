# ğŸ§  RAG-Enhanced LLM Chatbot  
An intelligent chatbot system built with **Streamlit** (frontend) and **FastAPI** (backend), combining **Retrieval-Augmented Generation (RAG)** and **general LLM reasoning**.  
It can switch seamlessly between document-based answers and open-domain reasoning, powered by **DeepSeek**, **OpenAI**, or **Gemini** APIs.

---

## ğŸ¬ Demo Video  
**Watch the app in action!**  
[![Demo Video](https://img.shields.io/badge/Demo%20Video-ğŸ¥-red)](https://drive.google.com/file/d/1gdHzO-lBwIDtDN4ioaMxsV0LVeTstewi/view?usp=drive_link)

---

## âœ¨ Features
- Hybrid **RAG + LLM** architecture  
- Toggle between **document-based** and **general** chatbot modes  
- Dynamic **system message customization** (personality & tone control)  
- Real-time **chat interface** with persistent history  
- Supports multiple **LLMs** (DeepSeek, OpenAI, Gemini)  
- Vector database management using **ChromaDB**  
- RAG context generation via **sentence-transformer embeddings**  
- Streamlit-based UI with configurable model parameters (temperature, max tokens)  
- Fully Dockerized for easy deployment  

---

## ğŸ§© Project Structure

```bash
chatbot/
â”œâ”€â”€ chatbot.py                     # Streamlit frontend main entry
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                     # FastAPI backend
â”‚   â”œâ”€â”€ config.py                  # Global configuration (models, constants)
â”‚   â”œâ”€â”€ rag_engine.py              # RAG logic (ChromaDB vector search)
â”‚   â”œâ”€â”€ app_logger.py              # Logging utilities
â”‚
â”œâ”€â”€ pages/                         # Streamlit multipage UI
â”‚   â”œâ”€â”€ LLM.py                     # LLM model configuration page
â”‚   â”œâ”€â”€ RAG.py                     # RAG control and database selection
â”‚   â”œâ”€â”€ Database_Manager.py        # Manage ChromaDB collections
â”‚   â””â”€â”€ System_Config.py           # Customize system messages & parameters
â”‚
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ Dockerfile.backend             # FastAPI backend image
â”œâ”€â”€ Dockerfile.frontend            # Streamlit frontend image
â”œâ”€â”€ docker-compose.yml             # Multi-container orchestration
â””â”€â”€ README.md                      # Project documentation

```

## System Requirements

Python 3.9+

Docker 24+

At least 4GB RAM

Internet connection for LLM APIs

API keys for one or more models:

-   DeepSeek

-   OpenAI

-   Google Gemini

## Getting Started

### Method 1: Local Development

1. Clone the repository
```bash
git clone https://github.com/Nidals17/Chatbot-AI.git
```

2. Set up environment variables:
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS / Linux
source venv/bin/activate
```

3. Install Dependencies:
```bash
pip install -r requirements.txt
```


4. Run Backend (FastAPI):
```bash
uvicorn backend.app:app --reload
```


â¡ï¸ Access FastAPI docs at http://127.0.0.1:8000/docs


5. Run Frontend(Streamlit):
```bash
streamlit run chatbot.py
```
â¡ï¸ Access UI at http://localhost:8501


### Method 2: ğŸ³ Docker Deployment (Recommended)

1. Build and Start Containers

```bash
docker-compose build
docker-compose up
```

2ï¸âƒ£ Access the Services

ğŸ–¥ï¸ Frontend (Streamlit) â†’ http://localhost:8501

âš™ï¸ Backend (FastAPI) â†’ http://localhost:8000/docs


## ğŸ§  Usage

1. Go to LLM Page â†’ Select model (DeepSeek, Gemini, OpenAI) and provide API key

2. Go to RAG Page â†’ Choose to enable/disable document-based retrieval

3. Go to System Config Page â†’ Adjust system message, temperature, and token limits

4. Go to Chatbot Page â†’ Start chatting!

    - If RAG is enabled, responses will be grounded in your document database

    - If RAG is disabled, the chatbot answers using general LLM knowledge


## âš™ï¸ Configuration
ğŸ”¹ Backend Settings (in backend/config.py)

```bash
deepseek_base_url = "https://api.deepseek.com/v1/chat/completions"
gemini_model = "gemini-pro"
openai_model = "gpt-3.5-turbo"
```

ğŸ”¹ RAG Embedding Settings (in backend/rag_engine.py)


```bash
embedding_model = "sentence-transformers/all-mpnet-base-v2"
chunk_size = 800
chunk_overlap = 100
```

## ğŸ§° Dependencies

Key libraries used in the project:

- fastapi==0.115.0
- uvicorn==0.30.0
- streamlit==1.38.0
- httpx==0.27.0
- pydantic==2.7.0
- langchain==0.2.7
- chromadb==0.4.24
- sentence-transformers==2.2.2
- google-generativeai==0.5.4
- openai==1.34.0

## ğŸ§ª API Endpoints

POST /query_llm

Query any supported model (DeepSeek / Gemini / OpenAI) with or without RAG context.

## âš¡ Performance Tips

Use smaller embedding models for faster RAG retrieval

Adjust chunk_size and overlap for document balance

Keep prompts short when using long context

Regularly clean old ChromaDB indexes

## License
[MIT License](LICENSE)